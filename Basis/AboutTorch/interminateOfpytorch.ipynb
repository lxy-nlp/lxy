{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "损失函数\n",
    "'''\n",
    "import torch.nn as nn\n",
    "criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "criterion = nn.SmoothL1Loss() # (-1,1)时平方差损失\n",
    "criterion = nn.BCELoss() #二分类交叉熵\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失 用于多分类\n",
    "criterion = nn.NLLLoss() # 负对数似然损失函数\n",
    "criterion = nn.NLLLoss2d() # 用于图片上有多个标签的情况\n",
    "sample = [[1,1],[1,2]]\n",
    "sample = torch.tensor(sample,dtype=torch.double)\n",
    "target = [[0,1],[2,3]]\n",
    "target = torch.tensor(target,dtype=torch.double)\n",
    "loss = criterion(sample,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 卷积操作\n",
    "'''\n",
    "# 一维卷积\n",
    "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "in_channels(int) – 输入信号的通道。在文本分类中，即为词向量的维度\n",
    "out_channels(int) – 卷积产生的通道。有多少个out_channels，就需要多少个1维卷积\n",
    "kernel_size(int or tuple) - 卷积核的尺寸，卷积核的大小为(k,)，第二个维度是由in_channels来决定的，所以实际上卷积大小为kernel_size*in_channels\n",
    "inputdata :[batch_size,seq_len,input_size]\n",
    "输入前 需要改变inputdata的形状  [batch_size,input_size,seq_len] 因为卷积核中的kernel_size的宽可以看成单词的维度,高可以看成每次卷积的单词数,也就是是说参与运算的是句子长度,所以需要将句子长度放到最后一个维度\n",
    "\n",
    "# 二维卷积\n",
    "torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "input: (batch,channels,H，W)\n",
    "weight(kernel): (out_channels, in_channels/groups, kH, kW) out_channels 卷积核的数量,in_channels 输入特征的维度\n",
    "stride: 滑动步长\n",
    "padding: 数据填充\n",
    "''' "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 具体实现\n",
    "# 一维卷积\n",
    "conv1 = nn.Conv1d(in_channels=256,out_channels=100,kernel_size=3)\n",
    "input = torch.randn(32,35,256)\n",
    "input = input.permute(0,2,1)\n",
    "output = conv1(input)\n",
    "\n",
    "# 二维卷积\n",
    "conv2 = nn.Conv2d(in_channels=3,kernel_size=(5,5),padding=0,out_channels=16)\n",
    "input = torch.randn(40,3,28,28)\n",
    "output=conv2(input)\n",
    "\n",
    "# 池化操作\n",
    "# 平均池化 \n",
    "y = F.avg_pool2d(output,kernel_size=2,stride=1)\n",
    "# 最大池化\n",
    "yy = F.max_pool2d(output,kernel_size=2,stride=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([40, 16, 23, 23])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "# 优化器\n",
    "# 随机梯度下降\n",
    "opti = optim.SGD(model.parameters(),lr=0.01,momentum=0.0)\n",
    "# opti = optim.RMSprop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "nn.LSTM(input_size,hidden_size,num_layers,bias,batch_first,dropout,bidirectional)\n",
    "\n",
    "input_size：输入数据X的特征值的数目。\n",
    "hidden_size：隐藏层的神经元数量，也就是隐藏层的特征数量。\n",
    "num_layers：循环神经网络的层数，默认值是 1。\n",
    "bias：默认为 True，如果为 false 则表示神经元不使用 bias 偏移参数。\n",
    "batch_first：如果设置为 True，则输入数据的维度中第一个维度就是 batch 值，默认为 False。默认情况下第一个维度是序列的长度，第二个维度才是 batch，第三个维度是特征数目。\n",
    "dropout：如果不为空，则表示最后跟一个 dropout 层抛弃部分数据，抛弃数据的比例由该参数指定。\n",
    "bidirectional ：布尔型，设置为 True 那么 LSTM 就是一个双向的 RNN 网络（双向是指同时可以往后影响参数），默认是 False。\n",
    "\n",
    "输入：input,(h_0,c_0) \n",
    "input --- [seq_len,batch_size,input_size]\n",
    "h_0 --- (num_layers * num_direction,batch,hidden_size)\n",
    "c_0 --- (num_layers * num_direction,batch,hidden_size)\n",
    "\n",
    "输出： output,(h_n,c_n)\n",
    "output (seq_len,batch,hidden_size*num_direction)\n",
    "h_n (num_layers * num_directions,batch,hidden_size)\n",
    "c_n (num_layers * num_directions,batch,hidden_size)\n",
    "'''\n",
    "rnn = nn.LSTM(10,20,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "tor",
   "language": "python",
   "display_name": "tor"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}